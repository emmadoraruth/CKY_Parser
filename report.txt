Emma Ziegellaub Eichler - edz2103@columbia.edu
COMS W4705 - Natural Language Processing
Programming Assignment 2 Report

4)
Part 1: To replace all instances of rare words (five or fewer occurrences) in the the input file (training data) with the special "_RARE_" symbol, run:
	"python nlp2.py rarify input_filename"
Specifically, run:
	"python nlp2.py rarify parse_train.dat"
	"python nlp2.py rarify parse_train_ver.dat"
Part 2: N/A
Part 3: N/A
Part 4: N/A

5)
Part 1: To run the CKY dynamic programming algorithm on the input file to generate the highest probability parse tree for each sentence (separated by newline characters), using the counts generated by the counts file and output the results to the predictions file,  run:
	"python nlp2.py cky input_filename counts_filename predictions_filename"
Thus, before running CKY, we must generate counts.  The input_filename should be "parse_dev.dat"; the other filenames do not matter.  For example, I ran:
	"python count_cfg_freqs.py parse_train.dat > cfg_original.counts"
	"python nlp2.py cky parse_dev.dat cfg_original.counts cky_predictions_original.txt"
My output file, "cky_predictions_original.txt", is included in this submission.

Part 2:
The performance for the CKY parser trained on "parse_train.dat" is as follows:

      Type       Total   Precision      Recall     F1 Score
===============================================================
         .         370     1.000        1.000        1.000
       ADJ         164     0.827        0.555        0.664
      ADJP          29     0.333        0.241        0.280
  ADJP+ADJ          22     0.542        0.591        0.565
       ADP         204     0.955        0.946        0.951
       ADV          64     0.694        0.531        0.602
      ADVP          30     0.333        0.133        0.190
  ADVP+ADV          53     0.756        0.642        0.694
      CONJ          53     1.000        1.000        1.000
       DET         167     0.988        0.976        0.982
      NOUN         671     0.752        0.842        0.795
        NP         884     0.632        0.529        0.576
    NP+ADJ           2     0.286        1.000        0.444
    NP+DET          21     0.783        0.857        0.818
   NP+NOUN         131     0.641        0.573        0.605
    NP+NUM          13     0.214        0.231        0.222
   NP+PRON          50     0.980        0.980        0.980
     NP+QP          11     0.667        0.182        0.286
       NUM          93     0.984        0.645        0.779
        PP         208     0.597        0.635        0.615
      PRON          14     1.000        0.929        0.963
       PRT          45     0.957        0.978        0.967
   PRT+PRT           2     0.400        1.000        0.571
        QP          26     0.647        0.423        0.512
         S         587     0.626        0.782        0.695
      SBAR          25     0.091        0.040        0.056
      VERB         283     0.683        0.799        0.736
        VP         399     0.559        0.594        0.576
   VP+VERB          15     0.250        0.267        0.258

     total        4664     0.715        0.715        0.715

Part 3:
The overall performance has an accuracy of just above 70%, with an F1-Score of 0.715.  However, the performance varies wildly between the different non-terminals.  It does the best on the period and conjunction symbols, '.' and 'CONJ', with F1-Scores of 1.000--in fact, over 17 times better than it does on 'SBAR' (its worst non-terminal), with an F1-Score of 0.056.  There are 29 nonterminal symbols; the parser performs worse than 0.500 on only seven of them.

Part 4:
The CKY algorithm makes use of a function, q_rules, as requested by the problem, which calculates maximum likelihood estimates for rules.  It takes as input three dictionaries (hashes) of binary rules, unary rules, and non-terminal symbols, each mapped to their respective counts.  As output, it produces a dictionary (hash) mapping each rule to its ML estimate.

6)
Part 1: There is no new code for this part (see Part 4 for explanation).  We simply run the same program from question (5) using different training data to generate counts; the input_filename should be "parse_dev_vert.dat"; the other filenames do not matter.  For example, I ran:
	"python count_cfg_freqs.py parse_train_vert.dat > cfg.counts"
	"python nlp2.py cky parse_dev.dat cfg.counts cky_predictions.txt"
My output file, "cky_predictions.txt", is included in this submission.

Part 2:
The performance for the CKY parser trained on "parse_train_vert.dat" is as follows:

      Type       Total   Precision      Recall     F1 Score
===============================================================
         .         370     1.000        1.000        1.000
       ADJ         164     0.689        0.622        0.654
      ADJP          29     0.324        0.414        0.364
  ADJP+ADJ          22     0.591        0.591        0.591
       ADP         204     0.960        0.951        0.956
       ADV          64     0.759        0.641        0.695
      ADVP          30     0.417        0.167        0.238
  ADVP+ADV          53     0.700        0.660        0.680
      CONJ          53     1.000        1.000        1.000
       DET         167     0.988        0.994        0.991
      NOUN         671     0.795        0.845        0.819
        NP         884     0.617        0.548        0.580
    NP+ADJ           2     0.333        0.500        0.400
    NP+DET          21     0.944        0.810        0.872
   NP+NOUN         131     0.610        0.656        0.632
    NP+NUM          13     0.375        0.231        0.286
   NP+PRON          50     0.980        0.980        0.980
     NP+QP          11     0.750        0.273        0.400
       NUM          93     0.914        0.688        0.785
        PP         208     0.623        0.635        0.629
      PRON          14     1.000        0.929        0.963
       PRT          45     1.000        0.933        0.966
   PRT+PRT           2     0.286        1.000        0.444
        QP          26     0.650        0.500        0.565
         S         587     0.704        0.814        0.755
      SBAR          25     0.667        0.400        0.500
      VERB         283     0.790        0.813        0.801
        VP         399     0.663        0.677        0.670
   VP+VERB          15     0.294        0.333        0.312

     total        4664     0.742        0.742        0.742

Part 3:
The overall performance improves by about 3% after vertical markovization (a noticeable but not huge increase) to an F1-Score of 0.742.  The parser maintains 100% accuracy on '.' and 'CONJ'; it also exhibits no change on two other non-terminals (pronouns, 'PRON', and noun phrases+pronouns, 'NP+PRON'), both of which already had accuracy over 96%.  Its accuracy decreases on only four non-terminals ('ADVP+ADV', 'NP+ADJ', 'PRT', and 'PRT+PRT'); on the other 21 non-terminals, its accuracy increases (although it still performs worse than 0.500 on seven non-terminals).  The best performance increase is seen in 'SBAR', which before vertical markovization had the worst accuracy, at under 6% (an F1-Score of 0.056).  With vertical markovization, it increases to 50% accuracy (an F1-Score of 0.500)--an improvement of over 892%!

Part 4:
In the original CKY program I wrote for question (5), the algorithm iterated over all non-terminals in the grammar, and for each non-terminal, iterated over all rules, checking if the start symbol matched the current non-terminal.  If so, it proceeded to iterate over all possible split points.  If not, it did nothing and moved on to the next rule.  This program took about 10 minutes to run for question (5) and 60 minutes to run for question (6).  To drastically improve the efficiency, I made one simple change to how the rules were stored.  Originally, I had a hash of binary rules and a hash of unary rules; the keys were the rules and the values were their q parameters.  In the improved version, I have hashes of hashes holding the binary and unary rules; the keys are non-termianals and the value are hashes, in which the keys are rules with that non-terminal as the start symbol and the values are their q parameters.  Now, the algorithm iterates over all non-terminals in the grammar, and for each non-terminal, iterates over all the rules within the hash for that non-terminal.  This modified program runs in under a minute for question (5) and under two minutes for question (6).  The reason for the drastic increase in efficiency is this:  while in the original algorithm, we saved ourselves the time of having to iterate over all split points for rules with the wrong start symbol, we still had to access every rule in order to check for the wrong start symbol, which meant the runtime was linear in the number of rules in the grammar.  This number is very large for question (5) and exponentially larger for question (6).  However, in the updated version, not only do we not iterate over all split points for rules with the wrong start symbol, by subdividing the rules by start symbol, we do not have to ever access rules with the wrong start symbol.  Now the runtime is linear in the number of rules in the grammar with the correct start symbol, which is a very small fraction of the total number of rules in question (5) and even smaller in question (6) (hence why the exponentially greater improvement in efficiency for question (6) compared to question (5).)  However, the reason I say there is no new code for this part is that there is no reason not to run the improved CKY for question (5) instead of the original, since it still significantly improves the runtime.  The instructions I gave for running CKY for both questions (5) and (6) are for this improved version.  However, in case you would like to see (or run-though I have already warned how long it will take!), I have included the original algorithm in a separate file in my submission.  The running instructions are identical, except instead of 'nlp2.py', use 'nlp2original.py'.  For example, for question (5):
	"python count_cfg_freqs.py parse_train.dat > cfg_original.counts"
	"python nlp2original.py cky parse_dev.dat cfg_original.counts cky_predictions_original.txt"
and for question (6):
	"python count_cfg_freqs.py parse_train_vert.dat > cfg.counts"
	"python nlp2original.py cky parse_dev.dat cfg.counts cky_predictions.txt"